[2026-02-25 15:27:02] INFO __init__.py:53: dynamo.nixl_connect: Utilizing CuPy to enable GPU acceleration.
[2026-02-25 15:27:02] INFO encode_worker_handler.py:39: Using cupy for array operations (GPU mode).
[2m2026-02-25T15:27:03.021801Z[0m [32m INFO[0m [2margs.parse_args[0m[2m:[0m Setting --distributed-executor-backend=mp for TP=1 to avoid UniProcExecutor GIL contention with NIXL connector
[2m2026-02-25T15:27:03.022251Z[0m [32m INFO[0m [2margs.create_kv_transfer_config[0m[2m:[0m Creating kv_transfer_config from --connector ['nixl']
[2m2026-02-25T15:27:03.022320Z[0m [32m INFO[0m [2margs.create_kv_events_config[0m[2m:[0m Using env-var DYN_VLLM_KV_EVENT_PORT=20080 to create kv_events_config
[2m2026-02-25T15:27:03.022353Z[0m [32m INFO[0m [2margs.overwrite_args[0m[2m:[0m Using kv_events_config for publishing vLLM kv events over zmq: KVEventsConfig(enable_kv_cache_events=True, publisher='zmq', endpoint='tcp://*:20080', replay_endpoint=None, buffer_steps=10000, hwm=100000, max_queue_size=100000, topic='') (use_kv_events=True)
[2m2026-02-25T15:27:03.842465Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m Initializing Kubernetes discovery backend
[2m2026-02-25T15:27:03.842615Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Initializing KubeDiscoveryClient: pod_name=gtc-demo-0-vllmworker-74pkv, instance_id=15293d6165a3dd, namespace=dynamo-workload, pod_uid=a837025d-3448-4917-ab02-2b0b03713396
[2m2026-02-25T15:27:03.842780Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Discovery daemon started
[2m2026-02-25T15:27:03.842844Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube::daemon[0m[2m:[0m Discovery daemon starting
[2m2026-02-25T15:27:03.842857Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Initializing NetworkManager with TCP request plane [3mmode[0m[2m=[0mtcp [3mhost[0m[2m=[0m172.16.19.146 [3mport[0m[2m=[0mOS-assigned
[2m2026-02-25T15:27:03.842884Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube::daemon[0m[2m:[0m Daemon watching EndpointSlices with labels: nvidia.com/dynamo-discovery-backend=kubernetes, nvidia.com/dynamo-discovery-enabled=true
[2m2026-02-25T15:27:03.842900Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube::daemon[0m[2m:[0m Daemon watching DynamoWorkerMetadata CRs in namespace: dynamo-workload
[2m2026-02-25T15:27:03.843406Z[0m [32m INFO[0m [2mdynamo_runtime::system_status_server[0m[2m:[0m [spawn_system_status_server] binding to: 0.0.0.0:9090
[2m2026-02-25T15:27:03.843432Z[0m [32m INFO[0m [2mdynamo_runtime::system_status_server[0m[2m:[0m [spawn_system_status_server] system status server bound to: 0.0.0.0:9090
[2m2026-02-25T15:27:03.843444Z[0m [32m INFO[0m [2mdynamo_runtime::distributed[0m[2m:[0m System status server started successfully on 0.0.0.0:9090
WARNING 02-25 15:27:04 [prometheus.py:31] Found PROMETHEUS_MULTIPROC_DIR was set by user. This directory must be wiped between vLLM runs or you will find inaccurate metrics. Unset the variable and vLLM will properly handle cleanup.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 02-25 15:27:11 [model.py:530] Resolved architecture: LlamaForCausalLM
INFO 02-25 15:27:11 [model.py:1545] Using max model len 131072
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
WARNING 02-25 15:27:11 [model.py:1358] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 02-25 15:27:11 [model.py:530] Resolved architecture: LlamaForCausalLM
INFO 02-25 15:27:11 [model.py:1545] Using max model len 131072
INFO 02-25 15:27:11 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 02-25 15:27:12 [model.py:530] Resolved architecture: LlamaForCausalLM
WARNING 02-25 15:27:12 [model.py:1869] Casting torch.float16 to torch.bfloat16.
INFO 02-25 15:27:12 [model.py:1545] Using max model len 2048
INFO 02-25 15:27:12 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=16384.
WARNING 02-25 15:27:12 [modelopt.py:345] Detected ModelOpt fp8 checkpoint (quant_algo=FP8). Please note that the format is experimental and could change.
INFO 02-25 15:27:12 [vllm.py:630] Asynchronous scheduling is enabled.
INFO 02-25 15:27:12 [vllm.py:637] Disabling NCCL for DP synchronization when using async scheduling.
WARNING 02-25 15:27:12 [vllm.py:983] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py and use --no-disable-hybrid-kv-cache-manager to start vLLM.
[2m2026-02-25T15:27:18.047185Z[0m [32m INFO[0m [2mcore.__init__[0m[2m:[0m Initializing a V1 LLM engine (v0.14.1) with config: model='/models/nvidia/Llama-3.3-70B-Instruct-FP8', speculative_config=SpeculativeConfig(method='eagle3', model='/models/yuhuili/EAGLE3-LLaMA3.3-Instruct-70B', num_spec_tokens=3), tokenizer='/models/nvidia/Llama-3.3-70B-Instruct-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8_e4m3, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=/models/nvidia/Llama-3.3-70B-Instruct-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [16384], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2m2026-02-25T15:27:18.047317Z[0m [33m WARN[0m [2mmultiproc_executor.set_multiprocessing_worker_envs[0m[2m:[0m Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2m2026-02-25T15:27:23.404970Z[0m [32m INFO[0m [2mparallel_state.init_distributed_environment[0m[2m:[0m world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:54561 backend=nccl
[2m2026-02-25T15:27:23.453344Z[0m [32m INFO[0m [2mparallel_state.initialize_model_parallel[0m[2m:[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2m2026-02-25T15:27:24.351929Z[0m [33m WARN[0m [2m__init__.build_logitsprocs[0m[2m:[0m min_p, logit_bias, and min_tokens parameters won't currently work with speculative decoding enabled.
[2m2026-02-25T15:27:24.465556Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Starting to load model /models/nvidia/Llama-3.3-70B-Instruct-FP8...
[2m2026-02-25T15:27:37.637460Z[0m [32m INFO[0m [2mcuda.get_attn_backend_cls[0m[2m:[0m Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN')
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:08<01:57,  8.39s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:16<01:50,  8.49s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:25<01:41,  8.47s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:33<01:33,  8.46s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:42<01:24,  8.48s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:50<01:16,  8.49s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:59<01:07,  8.49s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  53% Completed | 8/15 [01:07<00:59,  8.46s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  60% Completed | 9/15 [01:16<00:50,  8.42s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  67% Completed | 10/15 [01:24<00:42,  8.48s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  73% Completed | 11/15 [01:32<00:33,  8.40s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  80% Completed | 12/15 [01:41<00:25,  8.41s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  87% Completed | 13/15 [01:49<00:16,  8.40s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards:  93% Completed | 14/15 [01:58<00:08,  8.46s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards: 100% Completed | 15/15 [02:04<00:00,  7.84s/it]
[0;36m(Worker pid=264)[0;0m Loading safetensors checkpoint shards: 100% Completed | 15/15 [02:04<00:00,  8.31s/it]
[0;36m(Worker pid=264)[0;0m 
[2m2026-02-25T15:29:42.705514Z[0m [32m INFO[0m [2mdefault_loader.load_weights[0m[2m:[0m Loading weights took 124.75 seconds
[2m2026-02-25T15:29:42.733721Z[0m [33m WARN[0m [2mkv_cache.process_weights_after_loading[0m[2m:[0m Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2m2026-02-25T15:29:42.733884Z[0m [33m WARN[0m [2mkv_cache.process_weights_after_loading[0m[2m:[0m Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2m2026-02-25T15:29:43.107433Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Loading drafter model...
[0;36m(Worker pid=264)[0;0m Loading pt checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
[0;36m(Worker pid=264)[0;0m Loading pt checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.15s/it]
[0;36m(Worker pid=264)[0;0m Loading pt checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.15s/it]
[0;36m(Worker pid=264)[0;0m 
[2m2026-02-25T15:29:48.329366Z[0m [32m INFO[0m [2mdefault_loader.load_weights[0m[2m:[0m Loading weights took 5.20 seconds
[2m2026-02-25T15:29:49.319441Z[0m [32m INFO[0m [2meagle.load_model[0m[2m:[0m Detected EAGLE model with distinct embed_tokens weights. Keeping separate embedding weights from the target model.
[2m2026-02-25T15:29:50.054459Z[0m [32m INFO[0m [2meagle.load_model[0m[2m:[0m Detected EAGLE model with distinct lm_head weights. Keeping separate lm_head weights from the target model.
[2m2026-02-25T15:29:50.363974Z[0m [32m INFO[0m [2mgpu_model_runner.load_model[0m[2m:[0m Model loading took 70.64 GiB memory and 145.421731 seconds
[2m2026-02-25T15:30:04.564560Z[0m [32m INFO[0m [2mbackends.__call__[0m[2m:[0m Using cache directory: /home/dynamo/.cache/vllm/torch_compile_cache/28a4ec42c4/rank_0_0/backbone for vLLM's torch.compile
[2m2026-02-25T15:30:04.564877Z[0m [32m INFO[0m [2mbackends.__call__[0m[2m:[0m Dynamo bytecode transform time: 14.08 s
[2m2026-02-25T15:30:20.042672Z[0m [32m INFO[0m [2mbackends.compile[0m[2m:[0m Cache the graph of compile range (1, 16384) for later use
[2m2026-02-25T15:30:29.290776Z[0m [32m INFO[0m [2mbackends.compile[0m[2m:[0m Compiling a graph for compile range (1, 16384) takes 12.97 s
[2m2026-02-25T15:30:29.292440Z[0m [32m INFO[0m [2mmonitor.end_monitoring_torch_compile[0m[2m:[0m torch.compile takes 27.04 s in total
[2m2026-02-25T15:30:29.770051Z[0m [32m INFO[0m [2mbackends.__call__[0m[2m:[0m Using cache directory: /home/dynamo/.cache/vllm/torch_compile_cache/28a4ec42c4/rank_0_0/eagle_head for vLLM's torch.compile
[2m2026-02-25T15:30:29.770346Z[0m [32m INFO[0m [2mbackends.__call__[0m[2m:[0m Dynamo bytecode transform time: 0.35 s
[2m2026-02-25T15:30:36.834424Z[0m [32m INFO[0m [2mbackends.compile[0m[2m:[0m Compiling a graph for compile range (1, 16384) takes 6.89 s
[2m2026-02-25T15:30:36.834679Z[0m [32m INFO[0m [2mmonitor.end_monitoring_torch_compile[0m[2m:[0m torch.compile takes 34.28 s in total
[2m2026-02-25T15:30:44.265988Z[0m [32m INFO[0m [2mgpu_worker.determine_available_memory[0m[2m:[0m Available KV cache memory: 50.09 GiB
[2m2026-02-25T15:30:44.588183Z[0m [32m INFO[0m [2mkv_cache_utils._report_kv_cache_config[0m[2m:[0m GPU KV cache size: 324,224 tokens
[2m2026-02-25T15:30:44.588256Z[0m [32m INFO[0m [2mkv_cache_utils._report_kv_cache_config[0m[2m:[0m Maximum concurrency for 131,072 tokens per request: 2.47x
[2m2026-02-25T15:30:44.607802Z[0m [32m INFO[0m [2mnixl_connector[0m[2m:[0m NIXL is available
[2m2026-02-25T15:30:44.610253Z[0m [32m INFO[0m [2mfactory.create_connector[0m[2m:[0m Creating v1 connector with name: NixlConnector and engine_id: 8e991639-d136-4185-a8d0-b6c50612ec89
[2m2026-02-25T15:30:44.610294Z[0m [33m WARN[0m [2mbase.__init__[0m[2m:[0m Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[2m2026-02-25T15:30:44.610320Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL wrapper
[2m2026-02-25T15:30:44.610338Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL worker 8e991639-d136-4185-a8d0-b6c50612ec89
[2m2026-02-25T15:30:44.829129Z[0m [32m INFO[0m [2mnixl_connector.get_required_kvcache_layout[0m[2m:[0m NixlConnector setting KV cache layout to HND for better xfer performance.
[0;36m(Worker pid=264)[0;0m 2026-02-25 15:30:44 NIXL INFO    _api.py:363 Backend UCX was instantiated
[0;36m(Worker pid=264)[0;0m 2026-02-25 15:30:44 NIXL INFO    _api.py:253 Initialized NIXL agent: 27230e5d-7365-4bdb-a54c-d944ab57aaac
[2m2026-02-25T15:30:44.862645Z[0m [32m INFO[0m [2mnixl_connector.register_kv_caches[0m[2m:[0m Registering KV_Caches. use_mla: False, kv_buffer_device: cuda, use_host_buffer: False
[0;36m(Worker pid=264)[0;0m 2026-02-25 15:30:45,578 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(Worker pid=264)[0;0m 2026-02-25 15:30:45,601 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(Worker pid=264)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/49 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|â–         | 1/49 [00:01<01:25,  1.79s/it]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|â–Œ         | 3/49 [00:01<00:24,  1.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|â–Š         | 4/49 [00:02<00:17,  2.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|â–ˆ         | 5/49 [00:02<00:12,  3.40it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|â–ˆâ–        | 6/49 [00:02<00:10,  4.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|â–ˆâ–        | 7/49 [00:02<00:08,  5.02it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–‹        | 8/49 [00:02<00:07,  5.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 9/49 [00:02<00:06,  6.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|â–ˆâ–ˆ        | 10/49 [00:02<00:05,  6.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 11/49 [00:02<00:05,  7.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|â–ˆâ–ˆâ–       | 12/49 [00:03<00:04,  7.43it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 13/49 [00:03<00:04,  7.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|â–ˆâ–ˆâ–Š       | 14/49 [00:03<00:04,  7.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆ       | 15/49 [00:03<00:04,  7.62it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 16/49 [00:03<00:04,  7.59it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 18/49 [00:03<00:03,  9.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/49 [00:03<00:02, 10.32it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/49 [00:04<00:02, 10.87it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 24/49 [00:04<00:02, 11.14it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 26/49 [00:04<00:02, 10.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 28/49 [00:04<00:01, 10.80it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/49 [00:04<00:01, 10.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 32/49 [00:04<00:01, 10.48it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 34/49 [00:05<00:01, 11.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 36/49 [00:05<00:01, 11.61it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 38/49 [00:05<00:00, 11.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40/49 [00:05<00:00, 11.82it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 42/49 [00:05<00:00, 11.86it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 44/49 [00:05<00:00, 11.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/49 [00:06<00:00, 11.99it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 48/49 [00:06<00:00, 12.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:06<00:00,  7.72it/s]
[0;36m(Worker pid=264)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/33 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   6%|â–Œ         | 2/33 [00:00<00:02, 15.44it/s]Capturing CUDA graphs (decode, FULL):  12%|â–ˆâ–        | 4/33 [00:00<00:01, 16.20it/s]Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 6/33 [00:00<00:01, 16.19it/s]Capturing CUDA graphs (decode, FULL):  24%|â–ˆâ–ˆâ–       | 8/33 [00:00<00:01, 16.34it/s]Capturing CUDA graphs (decode, FULL):  30%|â–ˆâ–ˆâ–ˆ       | 10/33 [00:00<00:01, 16.10it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 12/33 [00:00<00:01, 15.82it/s]Capturing CUDA graphs (decode, FULL):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 14/33 [00:00<00:01, 15.35it/s]Capturing CUDA graphs (decode, FULL):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 16/33 [00:01<00:01, 14.81it/s]Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 18/33 [00:01<00:00, 16.11it/s]Capturing CUDA graphs (decode, FULL):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 20/33 [00:01<00:00, 17.13it/s]Capturing CUDA graphs (decode, FULL):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 22/33 [00:01<00:00, 17.88it/s]Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 24/33 [00:01<00:00, 18.14it/s]Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 27/33 [00:01<00:00, 18.58it/s]Capturing CUDA graphs (decode, FULL):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 29/33 [00:01<00:00, 18.79it/s]Capturing CUDA graphs (decode, FULL):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 32/33 [00:01<00:00, 19.56it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:01<00:00, 17.40it/s]
[2m2026-02-25T15:30:54.743164Z[0m [32m INFO[0m [2mgpu_model_runner.capture_model[0m[2m:[0m Graph capturing finished in 9 secs, took -3.90 GiB
[2m2026-02-25T15:30:54.773990Z[0m [32m INFO[0m [2mcore._initialize_kv_caches[0m[2m:[0m init engine (profile, create kv cache, warmup model) took 64.41 seconds
[2m2026-02-25T15:30:55.275248Z[0m [32m INFO[0m [2mnixl_connector[0m[2m:[0m NIXL is available
[2m2026-02-25T15:30:55.277089Z[0m [32m INFO[0m [2mfactory.create_connector[0m[2m:[0m Creating v1 connector with name: NixlConnector and engine_id: 8e991639-d136-4185-a8d0-b6c50612ec89
[2m2026-02-25T15:30:55.277130Z[0m [33m WARN[0m [2mbase.__init__[0m[2m:[0m Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[2m2026-02-25T15:30:55.277183Z[0m [32m INFO[0m [2mnixl_connector.__init__[0m[2m:[0m Initializing NIXL Scheduler 8e991639-d136-4185-a8d0-b6c50612ec89
[2m2026-02-25T15:30:55.277686Z[0m [32m INFO[0m [2mkv_events.__init__[0m[2m:[0m Starting ZMQ publisher thread
[2m2026-02-25T15:30:55.454942Z[0m [32m INFO[0m [2mvllm.__post_init__[0m[2m:[0m Asynchronous scheduling is enabled.
INFO 02-25 15:30:55 [nixl_connector.py:97] NIXL is available
[2m2026-02-25T15:30:55.586174Z[0m [32m INFO[0m [2mmain.setup_vllm_engine[0m[2m:[0m VllmWorker for /models/nvidia/Llama-3.3-70B-Instruct-FP8 has been initialized
[2m2026-02-25T15:30:55.586248Z[0m [32m INFO[0m [2mengine_monitor.__init__[0m[2m:[0m VllmEngineMonitor initialized and health check task started.
[2m2026-02-25T15:30:55.586299Z[0m [32m INFO[0m [2mmain.setup_kv_event_publisher[0m[2m:[0m KV event publisher for dp_rank=0 subscribing to vLLM at tcp://127.0.0.1:20080
[2m2026-02-25T15:30:55.586321Z[0m [32m INFO[0m [2mdynamo_llm::kv_router::publisher[0m[2m:[0m Initializing KvEventPublisher for worker 5956318114718685 in component backend
[2m2026-02-25T15:30:55.586383Z[0m [32m INFO[0m [2mmain.setup_kv_event_publisher[0m[2m:[0m Worker reading KV events for dp_rank=0 from tcp://127.0.0.1:20080
[2m2026-02-25T15:30:55.586767Z[0m [32m INFO[0m [2mmain.init[0m[2m:[0m Registered engine routes: /engine/sleep, /engine/wake_up
[2m2026-02-25T15:30:55.586818Z[0m [32m INFO[0m [2mmain.init[0m[2m:[0m Registering model with endpoint types: chat,completions
[2m2026-02-25T15:30:55.586877Z[0m [32m INFO[0m [2mmain.register_vllm_model[0m[2m:[0m Getting engine runtime configuration metadata from vLLM engine for chat,completions...
[2m2026-02-25T15:30:55.586919Z[0m [32m INFO[0m [2mmain.get_engine_cache_info[0m[2m:[0m Cache config values: {'num_gpu_blocks': 20264}
[2m2026-02-25T15:30:55.586946Z[0m [32m INFO[0m [2mmain.get_engine_cache_info[0m[2m:[0m Scheduler config values: {'max_num_seqs': 64, 'max_num_batched_tokens': 16384}
[2m2026-02-25T15:30:55.588537Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering event channel: namespace=dynamo-workload-gtc-demo, component=, topic=kv_metrics, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.603089Z[0m [32m INFO[0m [2mdynamo_runtime::transports::event_plane[0m[2m:[0m EventPublisher registered with discovery [3mtopic[0m[2m=[0mkv_metrics [3mtransport[0m[2m=[0mNats [3minstance_id[0m[2m=[0m5956318114718685
[2m2026-02-25T15:30:55.603110Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering model card: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=generate, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.612127Z[0m [32m INFO[0m [2m_core[0m[2m:[0m Registered base model '/models/nvidia/Llama-3.3-70B-Instruct-FP8' MDC
[2m2026-02-25T15:30:55.612931Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m Creating TCP request plane server [3mbind_addr[0m[2m=[0m172.16.19.146:0 [3mport_source[0m[2m=[0m"OS-assigned"
[2m2026-02-25T15:30:55.612946Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Initializing TCP server with dispatcher (concurrency=1500, queue=6000)
[2m2026-02-25T15:30:55.612955Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Started TCP worker dispatcher with concurrency limit 1500
[2m2026-02-25T15:30:55.612959Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Binding TCP server to 172.16.19.146:0
[2m2026-02-25T15:30:55.612985Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m TCP server bound successfully [3mrequested[0m[2m=[0m172.16.19.146:0 [3mactual[0m[2m=[0m172.16.19.146:34311
[2m2026-02-25T15:30:55.612994Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::manager[0m[2m:[0m TCP request plane server started [3mactual_addr[0m[2m=[0m172.16.19.146:34311 [3mactual_port[0m[2m=[0m34311
[2m2026-02-25T15:30:55.613051Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Registered endpoint 'generate' with shared TCP server on 172.16.19.146:34311
[2m2026-02-25T15:30:55.613081Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering endpoint: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=generate, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.613234Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Registered endpoint 'load_lora' with shared TCP server on 172.16.19.146:34311
[2m2026-02-25T15:30:55.613241Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Registered endpoint 'clear_kv_blocks' with shared TCP server on 172.16.19.146:34311
[2m2026-02-25T15:30:55.613244Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Registered endpoint 'unload_lora' with shared TCP server on 172.16.19.146:34311
[2m2026-02-25T15:30:55.613249Z[0m [32m INFO[0m [2mdynamo_runtime::pipeline::network::ingress::shared_tcp_endpoint[0m[2m:[0m Registered endpoint 'list_loras' with shared TCP server on 172.16.19.146:34311
[2m2026-02-25T15:30:55.621272Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering endpoint: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=list_loras, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.629441Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering endpoint: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=load_lora, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.637531Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering endpoint: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=clear_kv_blocks, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:55.646064Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::kube[0m[2m:[0m Registering endpoint: namespace=dynamo-workload-gtc-demo, component=backend, endpoint=unload_lora, instance_id=15293d6165a3dd
[2m2026-02-25T15:30:56.104390Z[0m [32m INFO[0m [2mdynamo_runtime::discovery::metadata[0m[2m:[0m Snapshot (seq=1): 1 instances, added=["15293d6165a3dd"], removed=[], updated=[]
