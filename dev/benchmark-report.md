# KV-Aware Routing Benchmark Report

## Setup

- **Hardware**: 1x GPU node with 8x NVIDIA H100 80GB GPUs (NVLink)
- **Model**: Llama 3.1 70B Instruct FP8 (`nvidia/Llama-3.1-70B-Instruct-FP8`)
- **Configuration**: 4 replicas at TP=2 (all 8 GPUs utilized), aggregated serving (no disaggregation)
- **Infrastructure**: NVIDIA Dynamo frontend + TensorRT-LLM backend, etcd, NATS
- **Cluster**: DigitalOcean Kubernetes (DOKS) 1.35, region ams3
- **Routing modes**: KV-aware (`DYN_ROUTER_MODE=kv`) vs Round-Robin (`DYN_ROUTER_MODE=round_robin`)

## Methodology

- **Runs**: 3 independent sweeps with identical parameters, results averaged
- **Concurrency levels**: 20, 30, 40, 50, 60, 70, 80, 100, 120 concurrent multi-turn conversations
- **Target RPS**: 25 (effective RPS limited by concurrency and response latency)
- **Per level**: 60s warmup + 300s measurement window (3 snapshots at 100s intervals, averaged)
- **Workload**: Synthetic multi-turn chat (3-5 turns per conversation, follow-up questions generated by 8B model)
- **Metrics source**: Prometheus (`loadgen_ttft_all_seconds` Summary, `dynamo_frontend_cached_tokens` for KV hit rate)
- **Between modes**: DGD patched to switch routing mode; frontend pod restarted to apply; workers retain model in GPU memory
- **Post-sweep**: Cluster restored to KV mode with all pods restarted for fresh KV tracking

### Input Files

| Run | File | Date |
|-----|------|------|
| 1 | `benchmark-sweep-20260217-125242.tsv` | 2026-02-17 12:52 UTC |
| 2 | `benchmark-sweep-20260217-152217.tsv` | 2026-02-17 15:22 UTC |
| 3 | `benchmark-sweep-20260217-172511.tsv` | 2026-02-17 17:25 UTC |

## Results

### Averaged TTFT by Routing Mode and Concurrency

| Concurrency | KV p50 (ms) | KV p95 (ms) | RR p50 (ms) | RR p95 (ms) | KV p95 Advantage |
|:-----------:|:-----------:|:-----------:|:-----------:|:-----------:|:----------------:|
| 20 | 218 | 330 | 237 | 427 | -97ms (23%) |
| 30 | 221 | 325 | 241 | 426 | -101ms (24%) |
| 40 | 230 | 385 | 241 | 435 | -50ms (12%) |
| 50 | 234 | 400 | 249 | 443 | -43ms (10%) |
| 60 | 240 | 417 | 258 | 452 | -35ms (8%) |
| 70 | 262 | 448 | 276 | 466 | -18ms (4%) |
| 80 | 272 | 460 | 290 | 477 | -18ms (4%) |
| 100 | 293 | 483 | 307 | 490 | -7ms (1%) |
| 120 | 327 | 496 | 324 | 496 | 0ms (0%) |

### 400ms p95 TTFT SLA Analysis

| Mode | Max Concurrency Under 400ms p95 |
|------|:-------------------------------:|
| **KV-Aware** | **50 concurrent conversations** |
| **Round-Robin** | **None** (exceeds 400ms at all tested levels) |

KV-aware routing is the only mode that meets the 400ms p95 TTFT SLA at any tested concurrency level. Round-robin exceeds 400ms p95 even at the lowest tested load (concurrency 20, p95=427ms).

At the SLA boundary (concurrency 50), KV-aware routing delivers p95 TTFT of 400ms vs round-robin's 443ms — a 43ms (10%) improvement.

### KV Cache Hit Rate Comparison

| Concurrency | KV Hit Rate (KV mode) | KV Hit Rate (RR mode) | Difference |
|:-----------:|:---------------------:|:---------------------:|:----------:|
| 20 | 96.2% | 83.4% | +12.8pp |
| 30 | 96.3% | 84.0% | +12.3pp |
| 40 | 97.8% | 85.0% | +12.8pp |
| 50 | 95.8% | 85.1% | +10.7pp |
| 60 | 96.8% | 85.2% | +11.6pp |
| 70 | 95.7% | 82.7% | +13.0pp |
| 80 | 95.8% | 81.8% | +14.0pp |
| 100 | 93.4% | 83.5% | +9.9pp |
| 120 | 95.6% | 84.7% | +10.9pp |

**Average**: KV-aware achieves **96.0% hit rate** vs round-robin's **83.8%** — a consistent **+12.2 percentage point** improvement across all concurrency levels.

### Throughput

Both modes achieve similar actual RPS at each concurrency level, confirming that KV-aware routing does not introduce throughput overhead. The system saturates at approximately 5.2 RPS (limited by the 4 TP=2 replicas serving 70B parameters).

## Key Takeaway

KV-aware routing delivers measurably lower tail latency (TTFT p95) compared to round-robin by directing multi-turn conversations to the replica already holding their cached KV state. Across three independent benchmark runs at nine concurrency levels, KV routing maintained a 96% cache hit rate (vs 84% for round-robin) and was the only routing mode to meet a 400ms p95 TTFT SLA — sustaining it up to 50 concurrent conversations. This improvement requires zero application changes: the optimization is entirely at the infrastructure routing layer, transparent to the OpenAI-compatible API contract.
