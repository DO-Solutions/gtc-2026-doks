apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gtc-demo
  namespace: dynamo-workload
  annotations:
    nvidia.com/enable-grove: "false"
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-nfs-pvc
  envs:
    - name: HF_HOME
      value: "/models"
  services:
    Frontend:
      dynamoNamespace: gtc-demo
      componentType: frontend
      replicas: 1
      volumeMounts:
        - name: model-nfs-pvc
          mountPoint: /models
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/dynamo-frontend:0.9.0
          env:
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
      envs:
        - name: DYN_ROUTER_MODE
          value: kv

    TrtllmWorker:
      dynamoNamespace: gtc-demo
      componentType: main
      replicas: 2
      envFromSecret: hf-token
      volumeMounts:
        - name: model-nfs-pvc
          mountPoint: /models
      resources:
        limits:
          gpu: "4"
      extraPodSpec:
        hostIPC: true
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        volumes:
          - name: cuda-compat
            hostPath:
              path: /usr/local/cuda-13.1/compat
              type: Directory
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.9.0
          securityContext:
            capabilities:
              add: ["IPC_LOCK"]
          ports:
            - name: system
              containerPort: 9090
              protocol: TCP
          env:
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: DYN_SYSTEM_PORT
              value: "9090"
          command:
            - bash
            - -c
            - export LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:$LD_LIBRARY_PATH && exec python3 -m dynamo.trtllm "$@"
            - --
          args:
            - --model-path
            - /models/nvidia/Llama-3.1-70B-Instruct-FP8
            - --tensor-parallel-size
            - "4"
            - --max-num-tokens
            - "8192"
            - --max-batch-size
            - "16"
            - --free-gpu-memory-fraction
            - "0.85"
            - --publish-events-and-metrics
            - --override-engine-args
            - '{"enable_chunked_prefill":true,"disable_overlap_scheduler":false,"trust_remote_code":true,"kv_cache_config":{"dtype":"fp8"}}'
          volumeMounts:
            - name: cuda-compat
              mountPath: /usr/local/cuda/compat/lib.real
              readOnly: true
