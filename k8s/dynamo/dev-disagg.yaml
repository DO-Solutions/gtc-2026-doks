apiVersion: nvidia.com/v1alpha1
kind: DynamoGraphDeployment
metadata:
  name: gtc-demo
  namespace: dynamo-workload
  annotations:
    nvidia.com/enable-grove: "false"
spec:
  backendFramework: trtllm
  pvcs:
    - name: model-nfs-pvc
  envs:
    - name: HF_HOME
      value: "/models"
  services:
    Frontend:
      dynamoNamespace: gtc-demo
      componentType: frontend
      replicas: 1
      volumeMounts:
        - name: model-nfs-pvc
          mountPoint: /models
      extraPodSpec:
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/dynamo-frontend:0.9.0
          env:
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
      envs:
        - name: DYN_ROUTER_MODE
          value: kv

    TrtllmPrefillWorker:
      dynamoNamespace: gtc-demo
      componentType: worker
      subComponentType: prefill
      replicas: 1
      scalingAdapter:
        enabled: true
      envFromSecret: hf-token
      volumeMounts:
        - name: model-nfs-pvc
          mountPoint: /models
      resources:
        limits:
          gpu: "1"
      extraPodSpec:
        hostIPC: ${NVLINK_ENABLED}
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        volumes:
          - name: cuda-compat
            hostPath:
              path: /usr/local/cuda-13.1/compat
              type: Directory
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.9.0
          securityContext:
            capabilities:
              add: ["IPC_LOCK"]
          env:
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: UCX_TLS
              value: "cuda_ipc,cuda_copy,tcp"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            - name: UCX_LOG_LEVEL
              value: "info"
          command:
            - bash
            - -c
            - export LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:$LD_LIBRARY_PATH && exec python3 -m dynamo.trtllm "$@"
            - --
          args:
            - --model-path
            - /models/nvidia/Llama-3.1-8B-Instruct-FP8
            - --disaggregation-mode
            - prefill
            - --tensor-parallel-size
            - "1"
            - --max-num-tokens
            - "8192"
            - --max-batch-size
            - "16"
            - --free-gpu-memory-fraction
            - "0.80"
            - --publish-events-and-metrics
            - --override-engine-args
            - '{"enable_chunked_prefill":true,"disable_overlap_scheduler":true,"trust_remote_code":true,"kv_cache_config":{"dtype":"fp8"},"cache_transceiver_config":{"backend":"NIXL","max_tokens_in_buffer":65536}}'
          volumeMounts:
            - name: cuda-compat
              mountPath: /usr/local/cuda/compat/lib.real
              readOnly: true

    TrtllmDecodeWorker:
      dynamoNamespace: gtc-demo
      componentType: worker
      subComponentType: decode
      replicas: 1
      scalingAdapter:
        enabled: true
      envFromSecret: hf-token
      volumeMounts:
        - name: model-nfs-pvc
          mountPoint: /models
      resources:
        limits:
          gpu: "1"
      extraPodSpec:
        hostIPC: ${NVLINK_ENABLED}
        tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
        volumes:
          - name: cuda-compat
            hostPath:
              path: /usr/local/cuda-13.1/compat
              type: Directory
        mainContainer:
          image: nvcr.io/nvidia/ai-dynamo/tensorrtllm-runtime:0.9.0
          securityContext:
            capabilities:
              add: ["IPC_LOCK"]
          env:
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid
            - name: UCX_TLS
              value: "cuda_ipc,cuda_copy,tcp"
            - name: UCX_RNDV_SCHEME
              value: "get_zcopy"
            - name: UCX_RNDV_THRESH
              value: "0"
            - name: UCX_LOG_LEVEL
              value: "info"
          command:
            - bash
            - -c
            - export LD_LIBRARY_PATH=/usr/local/cuda/compat/lib.real:$LD_LIBRARY_PATH && exec python3 -m dynamo.trtllm "$@"
            - --
          args:
            - --model-path
            - /models/nvidia/Llama-3.1-8B-Instruct-FP8
            - --disaggregation-mode
            - decode
            - --tensor-parallel-size
            - "1"
            - --max-num-tokens
            - "8192"
            - --max-batch-size
            - "16"
            - --free-gpu-memory-fraction
            - "0.80"
            - --publish-events-and-metrics
            - --override-engine-args
            - '{"enable_chunked_prefill":true,"disable_overlap_scheduler":false,"trust_remote_code":true,"kv_cache_config":{"dtype":"fp8"},"cache_transceiver_config":{"backend":"NIXL","max_tokens_in_buffer":65536}}'
          volumeMounts:
            - name: cuda-compat
              mountPath: /usr/local/cuda/compat/lib.real
              readOnly: true
